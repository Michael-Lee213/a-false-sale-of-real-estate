{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:16.269437Z","iopub.execute_input":"2025-01-23T11:25:16.269834Z","iopub.status.idle":"2025-01-23T11:25:16.286114Z","shell.execute_reply.started":"2025-01-23T11:25:16.269805Z","shell.execute_reply":"2025-01-23T11:25:16.284694Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fake-real-estate2/sample_submission.csv\n/kaggle/input/fake-real-estate2/train.csv\n/kaggle/input/fake-real-estate2/test.csv\n/kaggle/input/fake-real-estate7/sample_submission.csv\n/kaggle/input/fake-real-estate7/train.csv\n/kaggle/input/fake-real-estate7/test.csv\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# import joblib\nfrom datetime import datetime\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\n# from imblearn.over_sampling import SMOTE\n# from collections import Counter\n# from imblearn.under_sampling import RandomUnderSampler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\n# path = kagglehub.dataset_download(\"kaiyoo88/fake-real-estate\")\n\npath = \"/kaggle/input/fake-real-estate7\"\nprint(\"Path to dataset files:\", path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:19.236170Z","iopub.execute_input":"2025-01-23T11:25:19.236579Z","iopub.status.idle":"2025-01-23T11:25:19.242647Z","shell.execute_reply.started":"2025-01-23T11:25:19.236546Z","shell.execute_reply":"2025-01-23T11:25:19.241357Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/fake-real-estate7\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# ë°ì´í„° ë¡œë“œ\ntrain = pd.read_csv(f'{path}/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:20.968407Z","iopub.execute_input":"2025-01-23T11:25:20.968881Z","iopub.status.idle":"2025-01-23T11:25:21.004253Z","shell.execute_reply.started":"2025-01-23T11:25:20.968843Z","shell.execute_reply":"2025-01-23T11:25:21.003149Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"# Feature & Target ì„¤ì •\nx = train.drop(['ID', 'í—ˆìœ„ë§¤ë¬¼ì—¬ë¶€'], axis=1)\ny = train['í—ˆìœ„ë§¤ë¬¼ì—¬ë¶€']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:22.704572Z","iopub.execute_input":"2025-01-23T11:25:22.704978Z","iopub.status.idle":"2025-01-23T11:25:22.711677Z","shell.execute_reply.started":"2025-01-23T11:25:22.704947Z","shell.execute_reply":"2025-01-23T11:25:22.710321Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# 1) ê²°ì¸¡ì¹˜ ì²˜ë¦¬: KNN Imputer ì‚¬ìš© (ë” ì •ë°€í•œ ë°©ì‹)\nknn_imputer = KNNImputer(n_neighbors=5)  # K=5ë¡œ ì„¤ì •í•˜ì—¬ ê²°ì¸¡ì¹˜ ì˜ˆì¸¡\ncolumns_fill_knn = ['í•´ë‹¹ì¸µ', 'ì´ì¸µ', 'ì „ìš©ë©´ì ', 'ë°©ìˆ˜', 'ìš•ì‹¤ìˆ˜', 'ì´ì£¼ì°¨ëŒ€ìˆ˜']\nx[columns_fill_knn] = knn_imputer.fit_transform(x[columns_fill_knn])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:24.530309Z","iopub.execute_input":"2025-01-23T11:25:24.530669Z","iopub.status.idle":"2025-01-23T11:25:24.754099Z","shell.execute_reply.started":"2025-01-23T11:25:24.530643Z","shell.execute_reply":"2025-01-23T11:25:24.752645Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"## 2) Feature Engineering ì¶”ê°€\n\ndef create_new_features(x):\n    x = x.copy() \n    \n    # 1 ë‹¨ìœ„ë©´ì ë‹¹ ê°€ê²© (ã¡ë‹¹ ê°€ê²©)\n    x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'] = x['ë³´ì¦ê¸ˆ'] / x['ì „ìš©ë©´ì ']\n    x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].fillna(x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].median())\n    \n    # 2 ë³´ì¦ê¸ˆ ëŒ€ë¹„ ì›”ì„¸ ë¹„ìœ¨\n    x['ë³´ì¦ê¸ˆ_ì›”ì„¸ë¹„ìœ¨'] = x['ë³´ì¦ê¸ˆ'] / (x['ì›”ì„¸'] + 1)\n    x['ë³´ì¦ê¸ˆ_ì›”ì„¸ë¹„ìœ¨'].fillna(x['ë³´ì¦ê¸ˆ_ì›”ì„¸ë¹„ìœ¨'].median())\n    \n    # 3 ì¸µìˆ˜ ë¹„ìœ¨ (í•´ë‹¹ì¸µ / ì´ì¸µ)\n    x['ì¸µìˆ˜_ë¹„ìœ¨'] = x['í•´ë‹¹ì¸µ'] / x['ì´ì¸µ']\n    x['ì¸µìˆ˜_ë¹„ìœ¨'].fillna(x['ì¸µìˆ˜_ë¹„ìœ¨'].median())\n    \n    # 4 ê²Œì¬ì¼ ê´€ë ¨ Feature\n    x['ê²Œì¬ì¼'] = pd.to_datetime(x['ê²Œì¬ì¼'])\n    # x['ê²Œì¬ìš”ì¼'] = x['ê²Œì¬ì¼'].dt.weekday\n    x['ê²Œì¬ì¼_ì—°ë„'] = x['ê²Œì¬ì¼'].dt.year  # ì—°ë„\n    x['ê²Œì¬ì¼_ì›”'] = x['ê²Œì¬ì¼'].dt.month  # ì›”\n    x['ê²Œì¬ì¼_ìš”ì¼'] = x['ê²Œì¬ì¼'].dt.weekday  # ìš”ì¼ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)\n    x['ê²Œì¬ì¼_ê²½ê³¼ì¼'] = (datetime.today() - x['ê²Œì¬ì¼']).dt.days\n    # 'ê²Œì¬ì¼' ì›ë³¸ ì»¬ëŸ¼ ì œê±° (ë¶ˆí•„ìš”)\n    x = x.drop(columns=['ê²Œì¬ì¼'])\n    \n    # 5 ë°©í–¥ ê·¸ë£¹í™”\n    direction_map = {'ë™í–¥': 'ë™', 'ì„œí–¥': 'ì„œ', 'ë‚¨í–¥': 'ë‚¨', 'ë¶í–¥': 'ë¶', 'ë‚¨ë™í–¥': 'ë‚¨', 'ë¶ë™í–¥': 'ë¶'}\n    x['ë°©í–¥_ê·¸ë£¹'] = x['ë°©í–¥'].map(direction_map)\n    \n    # 6 ì´ìƒ ê°€ê²© íƒì§€ Feature\n    unit_price_mean = x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].mean()\n    unit_price_std = x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].std()\n    x['ê°€ê²©_ì´ìƒì¹˜'] = ((x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'] - unit_price_mean) / unit_price_std).abs()\n    \n    # 7 ì£¼ì°¨ ê°€ëŠ¥ ì—¬ë¶€ ìˆ˜ì¹˜ ë³€í™˜\n    x['ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€'] = x['ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€'].map({'ê°€ëŠ¥': 1, 'ë¶ˆê°€ëŠ¥': 0})\n    \n    # 8 ì›”ì„¸ + ê´€ë¦¬ë¹„ ì´ ë¹„ìš©\n    x['ì›”ì„¸_ì´ë¹„ìš©'] = x['ì›”ì„¸'] + x['ê´€ë¦¬ë¹„']\n    x['ì›”ì„¸_ì´ë¹„ìš©'].fillna(x['ì›”ì„¸_ì´ë¹„ìš©'].median())\n    \n    # 9 ê´€ë¦¬ë¹„ ë¹„ìœ¨ (ê´€ë¦¬ë¹„ / ì›”ì„¸)\n    x['ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = x['ê´€ë¦¬ë¹„'] / (x['ì›”ì„¸'] + 1)\n    x['ê´€ë¦¬ë¹„_ë¹„ìœ¨'].fillna(x['ê´€ë¦¬ë¹„_ë¹„ìœ¨'].median())\n    \n    # 10 ë°©ìˆ˜ ë°€ì§‘ë„ (ë°©ìˆ˜ / ì „ìš©ë©´ì ) & ìš•ì‹¤ ë°€ì§‘ë„ (ìš•ì‹¤ìˆ˜ / ì „ìš©ë©´ì )\n    x['ë°©ìˆ˜_ë°€ì§‘ë„'] = x['ë°©ìˆ˜'] / (x['ì „ìš©ë©´ì '] + 1)\n    x['ìš•ì‹¤_ë°€ì§‘ë„'] = x['ìš•ì‹¤ìˆ˜'] / (x['ì „ìš©ë©´ì '] + 1)\n    \n    # 11. í”Œë«í¼ë³„ í‰ê·  ë³´ì¦ê¸ˆ / ì›”ì„¸ ì°¨ì´\n    í”Œë«í¼_ë³´ì¦ê¸ˆí‰ê·  = x.groupby('ì œê³µí”Œë«í¼')['ë³´ì¦ê¸ˆ'].transform('mean')\n    í”Œë«í¼_ì›”ì„¸í‰ê·  = x.groupby('ì œê³µí”Œë«í¼')['ì›”ì„¸'].transform('mean')\n    \n    x['ì œê³µí”Œë«í¼_ë³´ì¦ê¸ˆì°¨ì´'] = x['ë³´ì¦ê¸ˆ'] - í”Œë«í¼_ë³´ì¦ê¸ˆí‰ê· \n    x['ì œê³µí”Œë«í¼_ì›”ì„¸ì°¨ì´'] = x['ì›”ì„¸'] - í”Œë«í¼_ì›”ì„¸í‰ê· \n    \n    return x\n\nx = create_new_features(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:27.072551Z","iopub.execute_input":"2025-01-23T11:25:27.072960Z","iopub.status.idle":"2025-01-23T11:25:27.108095Z","shell.execute_reply.started":"2025-01-23T11:25:27.072894Z","shell.execute_reply":"2025-01-23T11:25:27.106494Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"# ## 3) Label Encoding (ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜)\n# label_encode_cols = ['ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ë°©í–¥'] # 'ê²Œì¬ì¼', 'ë°©í–¥_ê·¸ë£¹'\n# label_encoders = {}\n# for col in label_encode_cols:\n#     le = LabelEncoder()\n#     x[col] = le.fit_transform(x[col].astype(str))\n#     label_encoders[col] = le  # ë‚˜ì¤‘ì— ë³€í™˜ì„ ìœ„í•´ ì €ì¥","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:19:26.282687Z","iopub.execute_input":"2025-01-23T11:19:26.283110Z","iopub.status.idle":"2025-01-23T11:19:26.288167Z","shell.execute_reply.started":"2025-01-23T11:19:26.283077Z","shell.execute_reply":"2025-01-23T11:19:26.286925Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"# # 4) One-Hot Encoding ì ìš©\n# one_hot_cols = ['ë§¤ë¬¼í™•ì¸ë°©ì‹' ] # 'ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€'\n# one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n# x_encoded = one_hot_encoder.fit_transform(x[one_hot_cols])\n# x_encoded_df = pd.DataFrame(x_encoded, columns=one_hot_encoder.get_feature_names_out(one_hot_cols), index=x.index)\n\n# # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© í›„ ê¸°ì¡´ ì—´ ì‚­ì œ\n# x = pd.concat([x.drop(columns=one_hot_cols), x_encoded_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:19:30.458951Z","iopub.execute_input":"2025-01-23T11:19:30.459358Z","iopub.status.idle":"2025-01-23T11:19:30.464181Z","shell.execute_reply.started":"2025-01-23T11:19:30.459327Z","shell.execute_reply":"2025-01-23T11:19:30.462977Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# 5) Train / Validation ë¶„í•  (Stratified ë°©ì‹)\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:19:33.779696Z","iopub.execute_input":"2025-01-23T11:19:33.780124Z","iopub.status.idle":"2025-01-23T11:19:33.793294Z","shell.execute_reply.started":"2025-01-23T11:19:33.780083Z","shell.execute_reply":"2025-01-23T11:19:33.792071Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def add_noise(df, noise_level=0.02): #0.05\n    numeric_cols = df.select_dtypes(include=[np.number]).columns  # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ\n    df[numeric_cols] = df[numeric_cols] * (1 + noise_level * np.random.randn(*df[numeric_cols].shape))\n    return df\n\nprint(\"Before Noise Injection:\", len(x_train))\nx_train = add_noise(x_train)\nprint(\"After Noise Injection:\", len(x_train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:25:30.591538Z","iopub.execute_input":"2025-01-23T11:25:30.592010Z","iopub.status.idle":"2025-01-23T11:25:30.611222Z","shell.execute_reply.started":"2025-01-23T11:25:30.591978Z","shell.execute_reply":"2025-01-23T11:25:30.609972Z"}},"outputs":[{"name":"stdout","text":"Before Noise Injection: 1961\nAfter Noise Injection: 1961\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"# def augment_minority_class(x_train, y_train, num_augments=2, noise_level=0.05):\n#     numeric_cols = x_train.select_dtypes(include=[np.number]).columns\n#     x_train_1 = x_train[y_train == 1].copy()  # ì†Œìˆ˜ í´ë˜ìŠ¤(1)ë§Œ ì„ íƒ\n#     augmented_data = []\n\n#     for _ in range(num_augments):\n#         x_aug = x_train_1.copy()\n#         x_aug[numeric_cols] = x_aug[numeric_cols] * (1 + noise_level * np.random.randn(*x_aug[numeric_cols].shape))\n#         augmented_data.append(x_aug)\n\n#     x_train = pd.concat([x_train] + augmented_data, axis=0).reset_index(drop=True)\n#     y_train = np.concatenate([y_train] + [np.ones(len(x_train_1))] * num_augments)  # ë ˆì´ë¸” ì¶”ê°€\n\n#     return x_train, y_train\n\n# print(\"Before Minority Augmentation:\", len(x_train))\n# x_train, y_train = augment_minority_class(x_train, y_train, num_augments=2)\n# print(\"After Minority Augmentation:\", len(x_train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:20:54.660732Z","iopub.execute_input":"2025-01-23T11:20:54.661169Z","iopub.status.idle":"2025-01-23T11:20:54.666321Z","shell.execute_reply.started":"2025-01-23T11:20:54.661139Z","shell.execute_reply":"2025-01-23T11:20:54.664828Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import f1_score\n\n# cat_features ì •ì˜\ncat_features = ['ë§¤ë¬¼í™•ì¸ë°©ì‹', 'ì¤‘ê°œì‚¬ë¬´ì†Œ', 'ì œê³µí”Œë«í¼', 'ë°©í–¥', 'ë°©í–¥_ê·¸ë£¹']\n\n# ğŸ›  ë²”ì£¼í˜• ì»¬ëŸ¼ í™•ì¸ ë° ëˆ„ë½ ì²˜ë¦¬\nfor col in cat_features:\n    if col not in x.columns:\n        print(f\"'{col}' ì»¬ëŸ¼ì´ xì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 'ë¯¸í™•ì¸'ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\")\n        x[col] = 'ë¯¸í™•ì¸'\n    x[col] = x[col].astype(str)\n\nfor col in cat_features:\n    if col not in x_val.columns:\n        print(f\"'{col}' ì»¬ëŸ¼ì´ x_valì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 'ë¯¸í™•ì¸'ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\")\n        x_val[col] = 'ë¯¸í™•ì¸'\n    x_val[col] = x_val[col].astype(str)\n\n# CatBoost ëª¨ë¸ ì´ˆê¸°í™”\ncat_model = CatBoostClassifier(\n    cat_features=cat_features,\n    auto_class_weights=\"Balanced\",  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìë™ ê· í˜•í™”\n    verbose=0\n)\n\n# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •\nparam_grid = {\n    'iterations': [500, 1000],    # ë°˜ë³µ íšŸìˆ˜\n    'depth': [6, 8, 10],          # íŠ¸ë¦¬ ê¹Šì´\n    'learning_rate': [0.03, 0.07], # í•™ìŠµë¥ \n    'l2_leaf_reg': [3, 5, 7],      # ì •ê·œí™” íŒŒë¼ë¯¸í„°\n}\n\n# Stratified K-Fold ì ìš©\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# GridSearchCV ì‹¤í–‰\ngrid_search = GridSearchCV(\n    estimator=cat_model,\n    param_grid=param_grid,\n    cv=skf,                   # âœ… Stratified K-Fold ì ìš©\n    scoring='f1_macro',       # âœ… Macro F1 Score ê¸°ì¤€\n    n_jobs=-1,                # âœ… ë³‘ë ¬ ì²˜ë¦¬\n    verbose=1                 # âœ… ì§„í–‰ìƒí™© ì¶œë ¥\n)\n\n# Grid Search ì‹¤í–‰\ngrid_search.fit(x, y)\n\n# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„±ëŠ¥ ì¶œë ¥\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best Macro F1-score:\", grid_search.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:31:29.962368Z","iopub.execute_input":"2025-01-23T11:31:29.963011Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 36 candidates, totalling 180 fits\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(cat_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ìµœì  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\nbest_model = grid_search.best_estimator_\ny_val_pred = best_model.predict(x_val)\n\nmacro_f1 = f1_score(y_val, y_val_pred, average='macro')\nprint(f\"Validation Macro F1-score: {macro_f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test ë°ì´í„° ë¡œë“œ\ntest = pd.read_csv(f'{path}/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test ê²°ì¸¡ê°’ ëŒ€ì²´\ntest[columns_fill_knn] = knn_imputer.transform(test[columns_fill_knn])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def create_new_features(x): \n#     # 1 ë‹¨ìœ„ë©´ì ë‹¹ ê°€ê²© (ã¡ë‹¹ ê°€ê²©)\n#     x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'] = x['ë³´ì¦ê¸ˆ'] / x['ì „ìš©ë©´ì ']\n#     x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].fillna(x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].median(), inplace=True)\n    \n#     # 2 ë³´ì¦ê¸ˆ ëŒ€ë¹„ ì›”ì„¸ ë¹„ìœ¨\n#     x['ë³´ì¦ê¸ˆ_ì›”ì„¸ë¹„ìœ¨'] = x['ë³´ì¦ê¸ˆ'] / (x['ì›”ì„¸'] + 1)\n#     x['ë³´ì¦ê¸ˆ_ì›”ì„¸ë¹„ìœ¨'].fillna(x['ë³´ì¦ê¸ˆ_ì›”ì„¸ë¹„ìœ¨'].median(), inplace=True)\n    \n#     # 3 ì¸µìˆ˜ ë¹„ìœ¨ (í•´ë‹¹ì¸µ / ì´ì¸µ)\n#     x['ì¸µìˆ˜_ë¹„ìœ¨'] = x['í•´ë‹¹ì¸µ'] / x['ì´ì¸µ']\n#     x['ì¸µìˆ˜_ë¹„ìœ¨'].fillna(x['ì¸µìˆ˜_ë¹„ìœ¨'].median(), inplace=True)\n    \n#     # 4 ê²Œì¬ì¼ ê´€ë ¨ Feature\n#     x['ê²Œì¬ì¼'] = pd.to_datetime(x['ê²Œì¬ì¼'])\n#     x['ê²Œì¬ìš”ì¼'] = x['ê²Œì¬ì¼'].dt.weekday\n#     x['ê²Œì¬ì¼_ê²½ê³¼ì¼'] = (datetime(2025, 1, 20) - x['ê²Œì¬ì¼']).dt.days\n    \n#     # 5 ë°©í–¥ ê·¸ë£¹í™”\n#     direction_map = {'ë™í–¥': 'ë™', 'ì„œí–¥': 'ì„œ', 'ë‚¨í–¥': 'ë‚¨', 'ë¶í–¥': 'ë¶', 'ë‚¨ë™í–¥': 'ë‚¨', 'ë¶ë™í–¥': 'ë¶'}\n#     x['ë°©í–¥_ê·¸ë£¹'] = x['ë°©í–¥'].map(direction_map)\n    \n#     # 6 ì´ìƒ ê°€ê²© íƒì§€ Feature\n#     unit_price_mean = x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].mean()\n#     unit_price_std = x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'].std()\n#     x['ê°€ê²©_ì´ìƒì¹˜'] = ((x['ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©'] - unit_price_mean) / unit_price_std).abs()\n    \n#     # 7 ì£¼ì°¨ ê°€ëŠ¥ ì—¬ë¶€ ìˆ˜ì¹˜ ë³€í™˜\n#     x['ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€'] = x['ì£¼ì°¨ê°€ëŠ¥ì—¬ë¶€'].map({'ê°€ëŠ¥': 1, 'ë¶ˆê°€ëŠ¥': 0})\n    \n#     # 8 ì›”ì„¸ + ê´€ë¦¬ë¹„ ì´ ë¹„ìš©\n#     x['ì›”ì„¸_ì´ë¹„ìš©'] = x['ì›”ì„¸'] + x['ê´€ë¦¬ë¹„']\n#     x['ì›”ì„¸_ì´ë¹„ìš©'].fillna(x['ì›”ì„¸_ì´ë¹„ìš©'].median(), inplace=True)\n    \n#     # 9 ê´€ë¦¬ë¹„ ë¹„ìœ¨ (ê´€ë¦¬ë¹„ / ì›”ì„¸)\n#     x['ê´€ë¦¬ë¹„_ë¹„ìœ¨'] = x['ê´€ë¦¬ë¹„'] / (x['ì›”ì„¸'] + 1)\n#     x['ê´€ë¦¬ë¹„_ë¹„ìœ¨'].fillna(x['ê´€ë¦¬ë¹„_ë¹„ìœ¨'].median(), inplace=True)\n    \n#     # 10 ë°©ìˆ˜ ë°€ì§‘ë„ (ë°©ìˆ˜ / ì „ìš©ë©´ì ) & ìš•ì‹¤ ë°€ì§‘ë„ (ìš•ì‹¤ìˆ˜ / ì „ìš©ë©´ì )\n#     x['ë°©ìˆ˜_ë°€ì§‘ë„'] = x['ë°©ìˆ˜'] / (x['ì „ìš©ë©´ì '] + 1)\n#     x['ìš•ì‹¤_ë°€ì§‘ë„'] = x['ìš•ì‹¤ìˆ˜'] / (x['ì „ìš©ë©´ì '] + 1)\n    \n#     # 11 í”Œë«í¼ë³„ í‰ê·  ë³´ì¦ê¸ˆ / ì›”ì„¸ ì°¨ì´\n#     í”Œë«í¼_ë³´ì¦ê¸ˆí‰ê·  = train.groupby('ì œê³µí”Œë«í¼')['ë³´ì¦ê¸ˆ'].mean()\n#     í”Œë«í¼_ì›”ì„¸í‰ê·  = train.groupby('ì œê³µí”Œë«í¼')['ì›”ì„¸'].mean()\n    \n#     x['ì œê³µí”Œë«í¼_ë³´ì¦ê¸ˆì°¨ì´'] = x['ë³´ì¦ê¸ˆ'] - x['ì œê³µí”Œë«í¼'].map(í”Œë«í¼_ë³´ì¦ê¸ˆí‰ê· )\n#     x['ì œê³µí”Œë«í¼_ì›”ì„¸ì°¨ì´'] = x['ì›”ì„¸'] - x['ì œê³µí”Œë«í¼'].map(í”Œë«í¼_ì›”ì„¸í‰ê· )\n#     return x\n\n# test = create_new_features(test)\n# test.head()\n\ntest = create_new_features(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Label Encoding \n# for col in label_encode_cols:\n#     if col in test.columns:\n#         le = label_encoders[col] \n#         test[col] = test[col].astype(str)\n#         unseen = set(test[col].unique()) - set(le.classes_) \n#         # unseen = []\n\n#         if unseen: # ëœ¬ê¸ˆí¬ê°€ ìˆë‹¤\n#             le.classes_ = np.append(le.classes_, list(unseen))\n#         test[col] = le.transform(test[col].astype(str))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # One-Hot Encoding\n# test_encoded = one_hot_encoder.transform(test[one_hot_cols])\n# test_encoded_df = pd.DataFrame(test_encoded, columns=one_hot_encoder.get_feature_names_out(one_hot_cols), index=test.index)\n\n# test = pd.concat([test.drop(columns=one_hot_cols), test_encoded_df], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in cat_features:\n    test[col] = test[col].astype(str)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.drop(columns=['ID'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = pd.Series(cat_model.predict(test)) #best_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('1:', pred.sum(), '| ratio:', (pred.sum()/len(pred)*100))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit = pd.read_csv(f'{path}/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit['í—ˆìœ„ë§¤ë¬¼ì—¬ë¶€'] = pred # ìš°ë¦¬ì˜ ì˜ˆì¸¡ ë„£ëŠ”ë‹¤\nsubmit.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit.to_csv('./baseline_submission_220908.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}